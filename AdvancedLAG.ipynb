{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWmeuFbyzQ0WNWTElt6rlD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiruneko/AdvancedLAG/blob/main/AdvancedLAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ULTLlA0x2Xo"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade langchain langchain-openai openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "assert api_key, \"Colabのシークレットに OPENAI_API_KEY を登録してください。\"\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "print(\"OpenAI API キーは設定されています ✅\")\n",
        "\n",
        "import os\n",
        "\n",
        "# Colab に登録したシークレットを取得\n",
        "api_key = os.environ[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "gmlB0e-1y_Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-core==0.3.0 langchain-openai==0.2.0 \\\n",
        "     langchain-community==0.3.0 GitPython==3.1.43 \\\n",
        "     langchain-chroma==0.1.4 tavily-python==0.5.0"
      ],
      "metadata": {
        "id": "sieGVHKCzF8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import GitLoader\n",
        "\n",
        "def file_filter(file_path: str) -> bool:\n",
        "  return file_path.endswith(\".mdx\")\n",
        "\n",
        "loader = GitLoader(\n",
        "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
        "    repo_path=\"./langchain\",\n",
        "    branch=\"master\",\n",
        "    file_filter=file_filter,\n",
        ")\n",
        "\n",
        "documents = loader.load()\n",
        "print(len(documents))"
      ],
      "metadata": {
        "id": "8fgKEZBizM5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain-chroma chromadb langchain-openai langchain-text-splitters tiktoken"
      ],
      "metadata": {
        "id": "5HKpzNKy1zYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# バッチ分割ユーティリティを定義（トークン合計で 30万未満に分割）\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def tok_len(s: str) -> int:\n",
        "    return len(enc.encode(s))\n",
        "\n",
        "def batches_by_token_limit(docs, limit=240_000):\n",
        "    batch, total = [], 0\n",
        "    for d in docs:\n",
        "        t = tok_len(d.page_content)\n",
        "        if total + t > limit and batch:\n",
        "            yield batch\n",
        "            batch, total = [], 0\n",
        "        batch.append(d)\n",
        "        total += t\n",
        "    if batch:\n",
        "        yield batch\n"
      ],
      "metadata": {
        "id": "0P4vDZigz9vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U --force-reinstall --no-cache-dir \\\n",
        "  \"langchain-core>=0.3.80\" \\\n",
        "  \"langchain-openai>=0.3.33\" \\\n",
        "  \"langchain>=0.3.10\" \\\n",
        "  \"langchain-community>=0.3.10\" \\\n",
        "  \"langchain-text-splitters>=0.3.2\""
      ],
      "metadata": {
        "id": "vRvi0v5jAWnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from chromadb import PersistentClient\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "PERSIST_DIR = \"./chroma_langchain_docs\"\n",
        "COLLECTION  = \"langchain_docs_mdx\"\n",
        "\n",
        "embedder = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# 永続クライアント経由でコレクションを開く（なければ作られる）\n",
        "client = PersistentClient(path=PERSIST_DIR)\n",
        "db = Chroma(client=client, collection_name=COLLECTION, embedding_function=embedder)\n",
        "\n",
        "print(\"DB ready\")"
      ],
      "metadata": {
        "id": "73g5_cyt_1Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH = 100\n",
        "for i in range(0, len(chunks), BATCH):\n",
        "    db.add_documents(chunks[i:i+BATCH])\n",
        "    print(f\"added batch {i//BATCH + 1}: {len(chunks[i:i+BATCH])} docs\")\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "print(\"OK: retriever ready\")\n"
      ],
      "metadata": {
        "id": "3ysoMLZw_M9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-openai"
      ],
      "metadata": {
        "id": "IPFm_rBf8l1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# ★ 追加: 前方参照を解決させる\n",
        "from langchain_core.caches import BaseCache  # noqa: F401 使わないが import だけ必要\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "ChatOpenAI.model_rebuild(force=True, raise_errors=False)  # ★ 一度だけ\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template('''\\\n",
        "以下の文脈だけを踏まえて質問に回答してください。\n",
        "\n",
        "文脈:\"\"\"\n",
        "{context}\n",
        "\"\"\"\n",
        "質問:{question}\n",
        "''')\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# retriever は List[Document] を返すので文字列化して渡すのが安全\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "chain = {\n",
        "    \"question\": RunnablePassthrough(),\n",
        "    \"context\": retriever | format_docs,   # ← ここで文字列化\n",
        "} | prompt | model | StrOutputParser()\n",
        "\n",
        "print(chain.invoke(\"LangChainの概要を教えて\"))\n"
      ],
      "metadata": {
        "id": "7R8xJ5nb0tJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip list | grep langchain"
      ],
      "metadata": {
        "id": "TXn9nHk_7iiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip list | grep pydantic"
      ],
      "metadata": {
        "id": "NEyU6pjY9j4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "arI137f-9n3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}